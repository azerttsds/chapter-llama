{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lutils import openf, writef\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add this import at the top of the notebook\n",
    "from typing import List, Dict\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(str(Path().cwd().parent.parent))\n",
    "from src.data.utils_asr import ChaptersASR\n",
    "from tools.captions.caption_selection import CaptionSelection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_furthest_frames_dict(frame_dict, target_count=100):\n",
    "    # Step 1: Parse the frame index from the key (assuming the format is 'frame_idx/total_frames')\n",
    "    frame_keys = list(frame_dict.keys())\n",
    "    frames = [(int(key.split(\"/\")[0]), key) for key in frame_keys]\n",
    "\n",
    "    # Step 2: Sort frames by their frame index\n",
    "    frames.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Step 3: Extract the frame indices for processing\n",
    "    frame_indices = [f[0] for f in frames]\n",
    "\n",
    "    # Step 4: Iteratively reduce frames until we have 100 left\n",
    "    while len(frame_indices) > target_count:\n",
    "        distances = [\n",
    "            (frame_indices[i + 1] - frame_indices[i], i)\n",
    "            for i in range(len(frame_indices) - 1)\n",
    "        ]\n",
    "\n",
    "        # Find the shortest distance and remove the corresponding frame\n",
    "        min_distance_idx = min(distances, key=lambda x: x[0])[1]\n",
    "\n",
    "        # Ensure min_distance_idx + 1 is within bounds\n",
    "        if (\n",
    "            min_distance_idx + 1 < len(distances)\n",
    "            and distances[min_distance_idx][0] <= distances[min_distance_idx + 1][0]\n",
    "        ):\n",
    "            frame_indices.pop(min_distance_idx + 1)\n",
    "            frames.pop(min_distance_idx + 1)\n",
    "        else:\n",
    "            frame_indices.pop(min_distance_idx)\n",
    "            frames.pop(min_distance_idx)\n",
    "\n",
    "    # Step 5: Rebuild the dictionary using the remaining keys\n",
    "    filtered_dict = {key: frame_dict[key] for _, key in frames}\n",
    "\n",
    "    return filtered_dict\n",
    "\n",
    "\n",
    "class CaptionSelector:\n",
    "    def __init__(\n",
    "        self,\n",
    "        captions_dir: Path,\n",
    "        vidc_dir: Path = Path(\"../../dataset/\"),\n",
    "        subsets: List[str] = (\"s1k_train\", \"s100_val\"),\n",
    "        max_gap: float = 2,\n",
    "    ):\n",
    "        subsets = [subsets] if isinstance(subsets, str) else subsets\n",
    "        self.subset2chp = {\n",
    "            subset: ChaptersASR(vidc_dir=vidc_dir, subset=subset) for subset in subsets\n",
    "        }\n",
    "        self.subsets = subsets\n",
    "        video_ids = []\n",
    "        for subset in subsets:\n",
    "            video_ids.extend(self.subset2chp[subset].video_ids)\n",
    "        # assert len(video_ids) == len(set(video_ids)), \"Duplicate video IDs\"\n",
    "        video_ids = list(set(video_ids))\n",
    "        self.video_ids = video_ids\n",
    "        self.captions_dir = captions_dir\n",
    "\n",
    "        self.vid2subset = {}\n",
    "        for subset in subsets:\n",
    "            for vid_id in self.subset2chp[subset].video_ids:\n",
    "                self.vid2subset[vid_id] = subset\n",
    "\n",
    "        self.max_gap = max_gap\n",
    "        self.vid2timestamps = defaultdict(list)\n",
    "\n",
    "    def get_chp(self, string: str):\n",
    "        if string in self.subsets:\n",
    "            return self.subset2chp[string]\n",
    "        elif string in self.vid2subset:\n",
    "            return self.subset2chp[self.vid2subset[string]]\n",
    "        else:\n",
    "            raise ValueError(f\"{string} not in subsets or vid2subset\")\n",
    "\n",
    "    def get_duration(self, string: str):\n",
    "        return self.get_chp(string).get_duration(string)\n",
    "\n",
    "    def get_captions(self, vid_id):\n",
    "        captions_file = self.captions_dir / f\"{vid_id[:2]}\" / f\"{vid_id}.json\"\n",
    "        all_captions = openf(captions_file)\n",
    "        return all_captions\n",
    "\n",
    "    def select_captions(\n",
    "        self, vid_id: str, timestamps: List[List[float]], max_captions: int = 100\n",
    "    ) -> Dict[str, str]:\n",
    "        vid_duration = self.get_duration(vid_id)\n",
    "        vid_captions = self.get_captions(vid_id)\n",
    "\n",
    "        selected_captions = {}\n",
    "\n",
    "        for timestamp in timestamps:\n",
    "            closest_frame = self.find_closest_frame(\n",
    "                timestamp, vid_duration, vid_captions\n",
    "            )\n",
    "            if closest_frame is None:\n",
    "                self.vid2timestamps[vid_id].append(timestamp)\n",
    "                continue\n",
    "\n",
    "            if closest_frame not in selected_captions:\n",
    "                selected_captions[closest_frame] = vid_captions[closest_frame]\n",
    "\n",
    "        if len(selected_captions) >= max_captions:\n",
    "            # filter captions, remove the ones if they are very close to each other\n",
    "            selected_captions = select_furthest_frames_dict(\n",
    "                selected_captions, max_captions\n",
    "            )\n",
    "\n",
    "        # sort by frame index\n",
    "        selected_captions = dict(\n",
    "            sorted(selected_captions.items(), key=lambda x: int(x[0].split(\"/\")[0]))\n",
    "        )\n",
    "\n",
    "        return selected_captions\n",
    "\n",
    "    def extract_timestamps(self, asr_segments: List[List[float]]) -> List[float]:\n",
    "        timestamps = []\n",
    "        for start, end in asr_segments:\n",
    "            timestamps.extend([start, end])\n",
    "        return sorted(set(timestamps))\n",
    "\n",
    "    def find_closest_frame(\n",
    "        self, timestamp: float, vid_duration: float, captions: Dict[str, str]\n",
    "    ) -> str:\n",
    "        target_ratio = timestamp / vid_duration\n",
    "        closest_frame = min(\n",
    "            captions.keys(),\n",
    "            key=lambda x: abs(\n",
    "                float(x.split(\"/\")[0]) / float(x.split(\"/\")[1]) - target_ratio\n",
    "            ),\n",
    "        )\n",
    "        # Verify the selected frame is within 2 seconds of target timestamp\n",
    "        frame_num, total_frames = map(float, closest_frame.split(\"/\"))\n",
    "        frame_timestamp = (frame_num / total_frames) * vid_duration\n",
    "        if abs(frame_timestamp - timestamp) > self.max_gap:\n",
    "            return None\n",
    "        return closest_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioner = \"HwwwH_MiniCPM-V-2\"\n",
    "captions_dir = Path(f\"../../dataset/captions/{captioner}/\")\n",
    "\n",
    "subsets = [\n",
    "    \"sml1k_train\",\n",
    "    \"sml10k_train\",\n",
    "    \"sml300_val\",\n",
    "]\n",
    "subsets = [\"sml10k_train\", \"sml300_val\"]\n",
    "subsets = [\"sml1k_train\", \"sml300_val\"]\n",
    "subsets = [\"sml_no-asr_val\"]\n",
    "cs = CaptionSelector(captions_dir / \"all\", subsets=subsets, max_gap=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s10k-2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../../\")\n",
    "vidc_dir = base_dir / \"dataset/\"\n",
    "\n",
    "subset_train = \"sml10k-2_train\"\n",
    "subset_train = \"s1k-2_train\"\n",
    "subset_train = \"sml1k_train\"\n",
    "subset_train = \"s10k-2_train\"\n",
    "\n",
    "selection = CaptionSelection(\n",
    "    vidc_dir=vidc_dir,\n",
    "    base_dir=base_dir,\n",
    "    sampling_methods=(\"asr-preds\",),\n",
    "    subset_train=subset_train,\n",
    ")\n",
    "\n",
    "method_flag = f\"asr_{subset_train}_preds\"\n",
    "\n",
    "\n",
    "captions_dir = Path(f\"../../dataset/captions/{captioner}/{method_flag}\")\n",
    "captions_dir.mkdir(exist_ok=True)\n",
    "\n",
    "missing_vids = []\n",
    "for vid_id in tqdm(cs.video_ids):\n",
    "    caption_pth = captions_dir / f\"{vid_id[:2]}\" / f\"{vid_id}.json\"\n",
    "    # if caption_pth.exists():\n",
    "    #     continue\n",
    "\n",
    "    vid_duration = cs.get_duration(vid_id)\n",
    "    no_preds_vid = []\n",
    "    try:\n",
    "        timestamps = selection(vid_id, duration=vid_duration)\n",
    "\n",
    "        selected_captions = cs.select_captions(vid_id, timestamps)\n",
    "        if not selected_captions:\n",
    "            no_preds_vid.append(vid_id)\n",
    "            continue\n",
    "        vid_caption_dir = captions_dir / f\"{vid_id[:2]}\"\n",
    "        vid_caption_dir.mkdir(exist_ok=True)\n",
    "        writef(vid_caption_dir / f\"{vid_id}.json\", selected_captions)\n",
    "    except Exception as e:\n",
    "        # print(f\"Error for {vid_id}: {e}\")\n",
    "        missing_vids.append(vid_id)\n",
    "        continue\n",
    "print(\n",
    "    f\"No preds for {len(no_preds_vid)} videos ({len(no_preds_vid) / len(cs.video_ids):.2%})\"\n",
    ")\n",
    "print(\n",
    "    f\"Missing {len(missing_vids)} videos ({len(missing_vids) / len(cs.video_ids):.2%})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:10<00:00, 17.75it/s]\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path(\"../../\")\n",
    "vidc_dir = base_dir / \"dataset/\"\n",
    "\n",
    "selection = CaptionSelection(\n",
    "    vidc_dir=vidc_dir,\n",
    "    base_dir=base_dir,\n",
    "    sampling_methods=(\"10s\",),\n",
    ")\n",
    "method_flag = \"10s\"\n",
    "\n",
    "\n",
    "captions_dir = Path(f\"../../dataset/captions/{captioner}/{method_flag}\")\n",
    "captions_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for vid_id in tqdm(cs.video_ids):\n",
    "    caption_pth = captions_dir / f\"{vid_id[:2]}\" / f\"{vid_id}.json\"\n",
    "    vid_duration = cs.get_duration(vid_id)\n",
    "    try:\n",
    "        timestamps = selection(vid_id, duration=vid_duration)\n",
    "\n",
    "        selected_captions = cs.select_captions(vid_id, timestamps)\n",
    "        vid_caption_dir = captions_dir / f\"{vid_id[:2]}\"\n",
    "        vid_caption_dir.mkdir(exist_ok=True)\n",
    "        writef(vid_caption_dir / f\"{vid_id}.json\", selected_captions)\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {vid_id}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shot detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../../\")\n",
    "vidc_dir = base_dir / \"dataset/\"\n",
    "\n",
    "sampling_method, shot_location = \"boundary\", \"boundaries\"\n",
    "sampling_method, shot_location = \"midpoint\", \"midpoints\"\n",
    "\n",
    "selection = CaptionSelection(\n",
    "    vidc_dir=vidc_dir,\n",
    "    base_dir=base_dir,\n",
    "    sampling_methods=(f\"shot-{sampling_method}\",),\n",
    ")\n",
    "\n",
    "method_flag = f\"shot_{shot_location}\"\n",
    "\n",
    "captions_dir = Path(f\"../../dataset/captions/{captioner}/{method_flag}\")\n",
    "captions_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for vid_id in tqdm(cs.video_ids):\n",
    "    caption_pth = captions_dir / f\"{vid_id[:2]}\" / f\"{vid_id}.json\"\n",
    "    vid_duration = cs.get_duration(vid_id)\n",
    "    try:\n",
    "        timestamps = selection(vid_id, duration=vid_duration)\n",
    "\n",
    "        selected_captions = cs.select_captions(vid_id, timestamps)\n",
    "        vid_caption_dir = captions_dir / f\"{vid_id[:2]}\"\n",
    "        vid_caption_dir.mkdir(exist_ok=True)\n",
    "        writef(vid_caption_dir / f\"{vid_id}.json\", selected_captions)\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {vid_id}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../../\")\n",
    "vidc_dir = base_dir / \"dataset/\"\n",
    "\n",
    "selection = CaptionSelection(\n",
    "    vidc_dir=vidc_dir,\n",
    "    base_dir=base_dir,\n",
    "    sampling_methods=(\"60s\",),\n",
    ")\n",
    "method_flag = \"60s\"\n",
    "\n",
    "\n",
    "captions_dir = Path(f\"../../dataset/captions/{captioner}/{method_flag}\")\n",
    "captions_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for vid_id in tqdm(cs.video_ids):\n",
    "    caption_pth = captions_dir / f\"{vid_id[:2]}\" / f\"{vid_id}.json\"\n",
    "    vid_duration = cs.get_duration(vid_id)\n",
    "    try:\n",
    "        timestamps = selection(vid_id, duration=vid_duration)\n",
    "\n",
    "        selected_captions = cs.select_captions(vid_id, timestamps)\n",
    "        vid_caption_dir = captions_dir / f\"{vid_id[:2]}\"\n",
    "        vid_caption_dir.mkdir(exist_ok=True)\n",
    "        writef(vid_caption_dir / f\"{vid_id}.json\", selected_captions)\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../../\")\n",
    "vidc_dir = base_dir / \"dataset/\"\n",
    "\n",
    "method_flag = \"100f\"\n",
    "method_flag = \"10f\"\n",
    "selection = CaptionSelection(\n",
    "    vidc_dir=vidc_dir,\n",
    "    base_dir=base_dir,\n",
    "    sampling_methods=(method_flag,),\n",
    ")\n",
    "\n",
    "\n",
    "captions_dir = Path(f\"../../dataset/captions/{captioner}/{method_flag}\")\n",
    "captions_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for vid_id in tqdm(cs.video_ids):\n",
    "    caption_pth = captions_dir / f\"{vid_id[:2]}\" / f\"{vid_id}.json\"\n",
    "    vid_duration = cs.get_duration(vid_id)\n",
    "    try:\n",
    "        timestamps = selection(vid_id, duration=vid_duration)\n",
    "\n",
    "        selected_captions = cs.select_captions(vid_id, timestamps)\n",
    "        vid_caption_dir = captions_dir / f\"{vid_id[:2]}\"\n",
    "        vid_caption_dir.mkdir(exist_ok=True)\n",
    "        assert len(selected_captions) == 10\n",
    "        writef(vid_caption_dir / f\"{vid_id}.json\", selected_captions)\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR + 10s if no ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: asr_s10k-2_train_preds+no-asr-10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:11<00:00, 16.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/lucas/datasets/VidChapters/captions/HwwwH_MiniCPM-V-2/asr_s10k-2_train_preds+no-asr-10s asr_s10k-2_train_preds+no-asr-10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "captioner = \"openbmb_MiniCPM-V-2_6\"\n",
    "captioner = \"HwwwH_MiniCPM-V-2\"\n",
    "\n",
    "caption_dir = Path(f\"../../dataset/captions/{captioner}/\")\n",
    "data1 = \"asr_sml10k-2_train_preds\"\n",
    "data1 = \"asr_sml1k_train_preds\"\n",
    "data1 = \"asr_s1k-2_train_preds\"\n",
    "data1 = \"asr_s10k-2_train_preds\"\n",
    "\n",
    "asr_preds_dir = caption_dir / data1\n",
    "\n",
    "data2 = \"10s\"\n",
    "data2_dir = Path(f\"../../dataset/captions/{captioner}/{data2}\")\n",
    "\n",
    "add_prefix = False\n",
    "\n",
    "# Create a directory for the combined approach\n",
    "combined_stem = asr_preds_dir.stem + f\"+no-asr-{data2}\"\n",
    "\n",
    "combined_stem += \"_captionsWithPrefix\" if add_prefix else \"\"\n",
    "\n",
    "print(f\"Data: {combined_stem}\")\n",
    "combined_dir = caption_dir / combined_stem\n",
    "combined_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for vid_id in tqdm(cs.video_ids):\n",
    "    vid_duration = cs.get_duration(vid_id)\n",
    "\n",
    "    # Get ASR captions\n",
    "    asr_pth = asr_preds_dir / f\"{vid_id[:2]}\" / f\"{vid_id}.json\"\n",
    "    s10_pth = data2_dir / f\"{vid_id[:2]}\" / f\"{vid_id}.json\"\n",
    "    if vid_id in cs.get_chp(vid_id) and asr_pth.exists():\n",
    "        vid_captions = openf(asr_pth)\n",
    "        if add_prefix:\n",
    "            vid_captions = {k: \"<speech> \" + v for k, v in vid_captions.items()}\n",
    "\n",
    "    # elif s10_pth.exists():\n",
    "    elif s10_pth.exists() and not asr_pth.exists():\n",
    "        vid_captions = openf(s10_pth)\n",
    "        if add_prefix:\n",
    "            vid_captions = {k: \"<visual> \" + v for k, v in vid_captions.items()}\n",
    "    else:\n",
    "        print(f\"{vid_id} has no captions\")\n",
    "        continue\n",
    "\n",
    "    vid_captions = select_furthest_frames_dict(vid_captions, 100)\n",
    "\n",
    "    # Write combined captions\n",
    "    combined_file = combined_dir / f\"{vid_id[:2]}\" / f\"{vid_id}.json\"\n",
    "    combined_file.parent.mkdir(exist_ok=True)\n",
    "    writef(combined_file, vid_captions)\n",
    "\n",
    "print(f\"{combined_dir.resolve()} {combined_stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proprietary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../../\")\n",
    "vidc_dir = base_dir / \"dataset/\"\n",
    "\n",
    "model = \"gpt-4o\"\n",
    "model = \"gemini-1.5-pro\"\n",
    "model = \"gpt-4o-mini\"\n",
    "model = \"gemini-2.0-flash\"\n",
    "selection = CaptionSelection(\n",
    "    vidc_dir=vidc_dir,\n",
    "    base_dir=base_dir,\n",
    "    sampling_methods=(\"asr-preds\",),\n",
    "    model=model,\n",
    "    subset_train=\"zero-shot\",\n",
    ")\n",
    "\n",
    "method_flag = f\"asr_{model}_zero-shot\"\n",
    "\n",
    "captions_dir = Path(f\"../../dataset/captions/{captioner}/{method_flag}\")\n",
    "captions_dir.mkdir(exist_ok=True)\n",
    "print(\"Saving data in:\", captions_dir.resolve())\n",
    "\n",
    "no_preds_vid = []\n",
    "for vid_id in tqdm(cs.video_ids):\n",
    "    caption_pth = captions_dir / f\"{vid_id[:2]}\" / f\"{vid_id}.json\"\n",
    "    # if caption_pth.exists():\n",
    "    #     continue\n",
    "\n",
    "    vid_duration = cs.get_duration(vid_id)\n",
    "    try:\n",
    "        timestamps = selection(vid_id, duration=vid_duration)\n",
    "\n",
    "        selected_captions = cs.select_captions(vid_id, timestamps)\n",
    "        if not selected_captions:\n",
    "            no_preds_vid.append(vid_id)\n",
    "            continue\n",
    "        vid_caption_dir = captions_dir / f\"{vid_id[:2]}\"\n",
    "        vid_caption_dir.mkdir(exist_ok=True)\n",
    "        writef(vid_caption_dir / f\"{vid_id}.json\", selected_captions)\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {vid_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\n",
    "    f\"No preds for {len(no_preds_vid)} videos ({len(no_preds_vid) / len(cs.video_ids):.2%})\"\n",
    ")\n",
    "\n",
    "files_in_captions_dir = list(captions_dir.glob(\"**/*.json\"))\n",
    "vids_in_captions_dir = {p.stem for p in files_in_captions_dir}\n",
    "vids_not_in_captions_dir = [\n",
    "    vid for vid in cs.video_ids if vid not in vids_in_captions_dir\n",
    "]\n",
    "\n",
    "print(len(vids_not_in_captions_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No ASR predictions from 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../../\")\n",
    "vidc_dir = base_dir / \"dataset/\"\n",
    "\n",
    "selection = CaptionSelection(\n",
    "    vidc_dir=vidc_dir,\n",
    "    base_dir=base_dir,\n",
    "    sampling_methods=(\"asr-preds\",),\n",
    "    data_flags=\"10s\",\n",
    "    prompt=\"captions\",\n",
    "    subset_train=\"s1k-2_no-asr_train\",\n",
    ")\n",
    "\n",
    "method_flag = \"captions10s_s1k-2_no-asr_train\"\n",
    "\n",
    "captions_dir = Path(f\"../../dataset/captions/{captioner}/{method_flag}\")\n",
    "captions_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for vid_id in tqdm(cs.video_ids):\n",
    "    caption_pth = captions_dir / f\"{vid_id[:2]}\" / f\"{vid_id}.json\"\n",
    "    # if caption_pth.exists():\n",
    "    #     continue\n",
    "\n",
    "    vid_duration = cs.get_duration(vid_id)\n",
    "    no_preds_vid = []\n",
    "    try:\n",
    "        timestamps = selection(vid_id, duration=vid_duration)\n",
    "\n",
    "        selected_captions = cs.select_captions(vid_id, timestamps)\n",
    "        if not selected_captions:\n",
    "            no_preds_vid.append(vid_id)\n",
    "            continue\n",
    "        vid_caption_dir = captions_dir / f\"{vid_id[:2]}\"\n",
    "        vid_caption_dir.mkdir(exist_ok=True)\n",
    "        writef(vid_caption_dir / f\"{vid_id}.json\", selected_captions)\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {vid_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\n",
    "    f\"No preds for {len(no_preds_vid)} videos ({len(no_preds_vid) / len(cs.video_ids):.2%})\"\n",
    ")\n",
    "print(f\"saved at {captions_dir.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delete",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
